{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "import os, sys\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'indices'=<generator object <genexpr> at 0x7f8ea3913990> of the transform datasets.arrow_dataset.Dataset.select couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    }
   ],
   "source": [
    "use_split = \"train\"\n",
    "passage_ds_dir = \"/mnt/nvme0/vortex_pipeline1/EVQA_passages/\"\n",
    "ds_dir = \"/mnt/nvme0/vortex_pipeline1/EVQA_data/\"\n",
    "passage_ds = load_dataset('parquet', data_files ={  \n",
    "                                            'train' : passage_ds_dir + 'train_passages-00000-of-00001.parquet',\n",
    "                                            'test'  : passage_ds_dir + 'test_passages-00000-of-00001.parquet',\n",
    "                                            })[use_split]\n",
    "\n",
    "ds = load_dataset('parquet', data_files ={  \n",
    "                                            'train' : ds_dir + 'train-00000-of-00001.parquet',\n",
    "                                            'test'  : ds_dir + 'test-00000-of-00001-2.parquet',\n",
    "                                            })[use_split].select(i for i in range(166900, 167000, 1)) \n",
    "\n",
    "passage_contents = passage_ds[\"passage_content\"]\n",
    "passage_ids = passage_ds[\"passage_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    " with open(\"search_result_dict.json\", \"r\") as f:\n",
    "        ranking_dict = json.load(f)\n",
    "\n",
    "qid_list = list(ranking_dict)\n",
    "ds_answers = ds[\"answers\"]\n",
    "pos_id_list = ds[\"pos_item_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ks = [5]\n",
    "\n",
    "recall_dict = defaultdict(list)\n",
    "result_dict = defaultdict(list)\n",
    "\n",
    "for i, (question_id, pos_ids) in enumerate(zip(qid_list, pos_id_list)):\n",
    "    retrieved_docs = ranking_dict[question_id]\n",
    "    retrieved_doc_scores = [doc[2] for doc in retrieved_docs]\n",
    "    retrieved_docs = [doc[0] for doc in retrieved_docs]\n",
    "    retrieved_doc_ids = [passage_ids[doc_idx] for doc_idx in retrieved_docs]\n",
    "    retrieved_doc_texts = [passage_contents[doc_idx] for doc_idx in retrieved_docs]\n",
    "\n",
    "    retrieved_doc_list = [\n",
    "        {\n",
    "            \"passage_id\": doc_id,\n",
    "            \"score\": score,\n",
    "        } for doc_id, score in zip(retrieved_doc_ids, retrieved_doc_scores)\n",
    "    ]\n",
    "    hit_list = []\n",
    "    # Get answers\n",
    "    \n",
    "    answers = ds_answers[i]\n",
    "    for retrieved_doc_text in retrieved_doc_texts:\n",
    "        found = False\n",
    "        for answer in answers:\n",
    "            if answer.strip().lower() in retrieved_doc_text.lower():\n",
    "                found = True\n",
    "        if found:\n",
    "            hit_list.append(1)\n",
    "        else:\n",
    "            hit_list.append(0)\n",
    "\n",
    "    # print(hit_list)\n",
    "    # input()\n",
    "    for K in Ks:\n",
    "        recall = float(np.max(np.array(hit_list[:K])))\n",
    "        recall_dict[f\"Pseudo Recall@{K}\"].append(recall)\n",
    "    \n",
    "    retrieved_doc_ids = [passage_ids[doc_idx] for doc_idx in retrieved_docs] \n",
    "    hit_list = []\n",
    "    for retrieved_doc_id in retrieved_doc_ids:\n",
    "        found = False\n",
    "        for pos_id in pos_ids:\n",
    "            if pos_id == retrieved_doc_id:\n",
    "                found = True\n",
    "        if found:\n",
    "            hit_list.append(1)\n",
    "        else:\n",
    "            hit_list.append(0)\n",
    "    for K in Ks:\n",
    "        recall = float(np.max(np.array(hit_list[:K])))\n",
    "        recall_dict[f\"Recall@{K}\"].append(recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(recall_dict['Pseudo Recall@5']) / len(recall_dict['Pseudo Recall@5'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(recall_dict['Recall@5']) / len(recall_dict['Recall@5'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
